//! Standalone CLI binary for running delta-kernel-rs benchmarks.
//!
//! This binary reads workload specifications from a directory (generated by
//! the Java WorkloadSpecGenerator in Quicksilver) and runs them against
//! Delta tables using the default engine. Results are written as a JSON
//! array to an output file, matching the `KernelBenchmarkResult` schema
//! expected by Quicksilver.
//!
//! Usage:
//! ```bash
//! workload-runner \
//!   --workload-dir /path/to/specs \
//!   --warmup 1 \
//!   --iterations 3 \
//!   --output /tmp/results.json
//! ```

use std::path::PathBuf;
use std::sync::Arc;
use std::time::Instant;

use clap::Parser;
use delta_kernel::benchmarks::models::{ReadOperationType, WorkloadSpecType};
use delta_kernel::benchmarks::{create_runner, load_all_workloads};
use delta_kernel::engine::default::executor::tokio::TokioBackgroundExecutor;
use delta_kernel::engine::default::DefaultEngine;
use serde::Serialize;

/// Delta Kernel Rust workload benchmark runner.
///
/// Reads workload specs from a directory and runs benchmarks against Delta tables.
/// Outputs results as JSON matching the Quicksilver KernelBenchmarkResult schema.
#[derive(Parser, Debug)]
#[command(name = "workload-runner")]
struct Args {
    /// Directory containing workload specifications (table_info.json + specs/)
    #[arg(long)]
    workload_dir: PathBuf,

    /// Number of warmup iterations (not measured)
    #[arg(long, default_value_t = 1)]
    warmup: u32,

    /// Number of measured iterations
    #[arg(long, default_value_t = 3)]
    iterations: u32,

    /// Output file path for JSON results
    #[arg(long)]
    output: PathBuf,
}

/// Benchmark result matching Quicksilver's KernelBenchmarkResult exactly.
#[derive(Debug, Serialize)]
#[serde(rename_all = "camelCase")]
struct BenchmarkResult {
    workload_name: String,
    spec_type: String,
    durations_ms: Vec<i64>,
    min_duration_ms: i64,
    max_duration_ms: i64,
    avg_duration_ms: f64,
    std_dev_duration_ms: f64,
    success: bool,
    error_message: Option<String>,
    custom_metrics: std::collections::HashMap<String, f64>,
}

/// Set up the default engine for a specific table URL.
///
/// Each table may use a different storage backend (local, S3, etc.), so we create
/// an engine per table URL so that `object_store` configures the correct backend
/// and picks up credentials from the environment.
///
/// For S3 URLs, we explicitly pass the AWS region from environment variables
/// since `object_store` defaults to us-east-1 if not configured, causing
/// redirect errors for buckets in other regions.
fn setup_engine_for_url(table_url: &str) -> Arc<DefaultEngine<TokioBackgroundExecutor>> {
    let url = delta_kernel::try_parse_uri(table_url)
        .unwrap_or_else(|e| panic!("Failed to parse table URL '{}': {}", table_url, e));

    // Build storage options, including region for S3 URLs
    let mut options: Vec<(&str, String)> = Vec::new();

    if url.scheme() == "s3" || url.scheme() == "s3a" {
        // Explicitly pass region to object_store â€” it may not pick up AWS_REGION
        // from the environment in all configurations (e.g., when using IMDS credentials)
        if let Ok(region) = std::env::var("AWS_REGION") {
            eprintln!("[INFO] Using AWS_REGION={} for S3 store", region);
            options.push(("region", region));
        } else if let Ok(region) = std::env::var("AWS_DEFAULT_REGION") {
            eprintln!("[INFO] Using AWS_DEFAULT_REGION={} for S3 store", region);
            options.push(("region", region));
        } else {
            eprintln!("[WARN] No AWS_REGION or AWS_DEFAULT_REGION set for S3 URL");
        }
    }

    let store = delta_kernel::engine::default::storage::store_from_url_opts(&url, options)
        .unwrap_or_else(|e| panic!("Failed to create store for '{}': {}", table_url, e));
    Arc::new(DefaultEngine::new(store))
}

/// Normalize s3a:// URIs to s3:// since `object_store` uses s3://.
fn normalize_table_path(path: &str) -> String {
    if path.starts_with("s3a://") {
        format!("s3://{}", &path[6..])
    } else {
        path.to_string()
    }
}

fn compute_stats(durations: &[i64]) -> (i64, i64, f64, f64) {
    if durations.is_empty() {
        return (0, 0, 0.0, 0.0);
    }
    let min = *durations.iter().min().unwrap();
    let max = *durations.iter().max().unwrap();
    let sum: i64 = durations.iter().sum();
    let avg = sum as f64 / durations.len() as f64;
    let variance = durations
        .iter()
        .map(|&d| {
            let diff = d as f64 - avg;
            diff * diff
        })
        .sum::<f64>()
        / durations.len() as f64;
    let stddev = variance.sqrt();
    (min, max, avg, stddev)
}

fn main() {
    let args = Args::parse();

    eprintln!("=== Delta Kernel Rust Workload Runner ===");
    eprintln!("  Workload dir: {}", args.workload_dir.display());
    eprintln!("  Warmup: {}", args.warmup);
    eprintln!("  Iterations: {}", args.iterations);
    eprintln!("  Output: {}", args.output.display());
    eprintln!("  Working directory: {:?}", std::env::current_dir().ok());

    // Log relevant environment variables for debugging credential/config issues
    log_environment();

    // Log the contents of the workload directory for debugging
    log_workload_dir(&args.workload_dir);

    // Load all workload specs
    let specs = match load_all_workloads(&args.workload_dir) {
        Ok(specs) => specs,
        Err(e) => {
            eprintln!("[ERROR] Failed to load workload specs: {}", e);
            let results = vec![BenchmarkResult {
                workload_name: "all".to_string(),
                spec_type: "unknown".to_string(),
                durations_ms: vec![],
                min_duration_ms: 0,
                max_duration_ms: 0,
                avg_duration_ms: 0.0,
                std_dev_duration_ms: 0.0,
                success: false,
                error_message: Some(format!("Failed to load specs: {}", e)),
                custom_metrics: std::collections::HashMap::new(),
            }];
            write_results(&args.output, &results);
            std::process::exit(1);
        }
    };

    eprintln!("[INFO] Loaded {} workload spec(s)", specs.len());
    for (i, spec) in specs.iter().enumerate() {
        let table_root = spec.table_info.resolved_table_root();
        eprintln!(
            "[INFO]   [{}] table={}, case={}, type={:?}, table_path={:?}, resolved_root={}",
            i,
            spec.table_info.name,
            spec.case_name,
            match &spec.spec_type {
                WorkloadSpecType::Read(_) => "read",
                WorkloadSpecType::SnapshotConstruction(_) => "snapshot_construction",
            },
            spec.table_info.table_path,
            table_root
        );
    }

    let mut results = Vec::new();

    for spec in specs {
        // Normalize s3a:// paths and prepare spec
        let mut spec = spec;
        if let Some(ref path) = spec.table_info.table_path {
            let normalized = normalize_table_path(path);
            if normalized != *path {
                eprintln!("[INFO] Normalized path: {} -> {}", path, normalized);
                spec.table_info.table_path = Some(normalized);
            }
        }

        match &spec.spec_type {
            WorkloadSpecType::Read(read_spec) => {
                let op_type = read_spec
                    .operation_type
                    .as_deref()
                    .unwrap_or("read_metadata");

                let operation = match op_type {
                    "read_metadata" => ReadOperationType::ReadMetadata,
                    "read_data" => {
                        eprintln!("[WARN] Skipping read_data (not yet implemented)");
                        continue;
                    }
                    other => {
                        eprintln!("[WARN] Unknown read operation type: {}", other);
                        continue;
                    }
                };

                let mut spec_with_op = spec.clone();
                spec_with_op.operation_type = Some(operation);

                let table_url = spec_with_op.table_info.resolved_table_root();
                eprintln!("[INFO] Creating engine for table URL: {}", table_url);
                let engine = setup_engine_for_url(&table_url);

                let result = run_benchmark(&spec_with_op, engine, &args);
                results.push(result);
            }
            WorkloadSpecType::SnapshotConstruction(_) => {
                let table_url = spec.table_info.resolved_table_root();
                eprintln!("[INFO] Creating engine for table URL: {}", table_url);
                let engine = setup_engine_for_url(&table_url);

                let result = run_benchmark(&spec, engine, &args);
                results.push(result);
            }
        }
    }

    write_results(&args.output, &results);
    eprintln!(
        "=== Done: {} result(s) written to {} ===",
        results.len(),
        args.output.display()
    );
}

/// Log relevant environment variables for debugging.
fn log_environment() {
    eprintln!("[INFO] Environment:");
    let vars_to_check = [
        "AWS_ACCESS_KEY_ID",
        "AWS_SECRET_ACCESS_KEY",
        "AWS_SESSION_TOKEN",
        "AWS_REGION",
        "AWS_DEFAULT_REGION",
        "AWS_CONTAINER_CREDENTIALS_RELATIVE_URI",
        "HOME",
        "PATH",
    ];
    for var in &vars_to_check {
        match std::env::var(var) {
            Ok(val) => {
                // Mask sensitive values
                if var.contains("SECRET") || var.contains("TOKEN") || var.contains("KEY") {
                    eprintln!("  {}=<set, {} chars>", var, val.len());
                } else {
                    let display = if val.len() > 120 {
                        format!("{}...", &val[..120])
                    } else {
                        val
                    };
                    eprintln!("  {}={}", var, display);
                }
            }
            Err(_) => eprintln!("  {}=<not set>", var),
        }
    }
    // Check if IMDS (EC2 instance metadata) is likely available
    eprintln!(
        "  AWS_EC2_METADATA_DISABLED={}",
        std::env::var("AWS_EC2_METADATA_DISABLED").unwrap_or_else(|_| "<not set>".to_string())
    );
}

/// Log the contents of the workload directory for debugging.
fn log_workload_dir(dir: &std::path::Path) {
    eprintln!("[INFO] Workload directory contents:");
    if let Ok(entries) = std::fs::read_dir(dir) {
        for entry in entries.flatten() {
            let path = entry.path();
            let kind = if path.is_dir() { "DIR" } else { "FILE" };
            eprintln!("  [{}] {}", kind, path.display());

            // If it's a table directory, log its contents
            if path.is_dir() {
                if let Ok(sub_entries) = std::fs::read_dir(&path) {
                    for sub in sub_entries.flatten() {
                        let sub_path = sub.path();
                        let sub_kind = if sub_path.is_dir() { "DIR" } else { "FILE" };
                        eprintln!("    [{}] {}", sub_kind, sub_path.display());
                    }
                }
                // Log table_info.json content if present
                let table_info = path.join("table_info.json");
                if table_info.exists() {
                    if let Ok(content) = std::fs::read_to_string(&table_info) {
                        eprintln!("    table_info.json: {}", content.trim());
                    }
                }
                // Log spec files
                let specs_dir = path.join("specs");
                if specs_dir.is_dir() {
                    if let Ok(spec_entries) = std::fs::read_dir(&specs_dir) {
                        for spec_entry in spec_entries.flatten() {
                            let spec_path = spec_entry.path();
                            if spec_path.is_dir() {
                                let spec_file = spec_path.join("spec.json");
                                if spec_file.exists() {
                                    if let Ok(content) = std::fs::read_to_string(&spec_file) {
                                        eprintln!(
                                            "    specs/{}/spec.json: {}",
                                            spec_path.file_name().unwrap().to_string_lossy(),
                                            content.trim()
                                        );
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    } else {
        eprintln!("  [ERROR] Cannot read directory: {}", dir.display());
    }
}

fn run_benchmark(
    spec: &delta_kernel::benchmarks::WorkloadSpecVariant,
    engine: Arc<DefaultEngine<TokioBackgroundExecutor>>,
    args: &Args,
) -> BenchmarkResult {
    let workload_name = spec.full_name();
    let spec_type = match &spec.spec_type {
        WorkloadSpecType::Read(_) => "read",
        WorkloadSpecType::SnapshotConstruction(_) => "snapshot_construction",
    };
    let table_root = spec.table_info.resolved_table_root();

    eprintln!("----------------------------------------------------------------------");
    eprintln!("[INFO] Running benchmark: {} (type={})", workload_name, spec_type);
    eprintln!("[INFO]   Table: {} (path={})", spec.table_info.name, table_root);
    eprintln!("[INFO]   Warmup: {}, Iterations: {}", args.warmup, args.iterations);

    let mut runner = match create_runner(spec.clone(), engine) {
        Ok(r) => {
            eprintln!("[INFO]   Runner created successfully");
            r
        }
        Err(e) => {
            eprintln!("[ERROR]  Failed to create runner: {}", e);
            return BenchmarkResult {
                workload_name,
                spec_type: spec_type.to_string(),
                durations_ms: vec![],
                min_duration_ms: 0,
                max_duration_ms: 0,
                avg_duration_ms: 0.0,
                std_dev_duration_ms: 0.0,
                success: false,
                error_message: Some(format!("Failed to create runner: {}", e)),
                custom_metrics: std::collections::HashMap::new(),
            };
        }
    };

    // Warmup iterations
    for i in 0..args.warmup {
        eprintln!("[INFO]   Warmup {}/{}", i + 1, args.warmup);
        if let Err(e) = runner.setup() {
            eprintln!("[ERROR]  Setup failed during warmup: {}", e);
            return error_result(workload_name, spec_type, &format!("Setup failed: {}", e));
        }
        if let Err(e) = runner.execute() {
            eprintln!("[ERROR]  Execute failed during warmup: {}", e);
            return error_result(workload_name, spec_type, &format!("Execute failed: {}", e));
        }
        let _ = runner.cleanup();
    }

    // Measured iterations
    let mut durations_ms = Vec::with_capacity(args.iterations as usize);
    for i in 0..args.iterations {
        if let Err(e) = runner.setup() {
            eprintln!("[ERROR]  Setup failed at iteration {}: {}", i + 1, e);
            return error_result(workload_name, spec_type, &format!("Setup failed: {}", e));
        }

        let start = Instant::now();
        if let Err(e) = runner.execute() {
            eprintln!("[ERROR]  Execute failed at iteration {}: {}", i + 1, e);
            return error_result(workload_name, spec_type, &format!("Execute failed: {}", e));
        }
        let elapsed = start.elapsed();
        let ms = elapsed.as_millis() as i64;
        durations_ms.push(ms);
        eprintln!("[INFO]   Iteration {}/{}: {}ms", i + 1, args.iterations, ms);

        let _ = runner.cleanup();
    }

    let (min, max, avg, stddev) = compute_stats(&durations_ms);

    eprintln!(
        "[INFO]   Result: avg={:.1}ms, min={}ms, max={}ms, stddev={:.2}ms",
        avg, min, max, stddev
    );

    BenchmarkResult {
        workload_name,
        spec_type: spec_type.to_string(),
        durations_ms,
        min_duration_ms: min,
        max_duration_ms: max,
        avg_duration_ms: avg,
        std_dev_duration_ms: stddev,
        success: true,
        error_message: None,
        custom_metrics: std::collections::HashMap::new(),
    }
}

fn error_result(workload_name: String, spec_type: &str, message: &str) -> BenchmarkResult {
    BenchmarkResult {
        workload_name,
        spec_type: spec_type.to_string(),
        durations_ms: vec![],
        min_duration_ms: 0,
        max_duration_ms: 0,
        avg_duration_ms: 0.0,
        std_dev_duration_ms: 0.0,
        success: false,
        error_message: Some(message.to_string()),
        custom_metrics: std::collections::HashMap::new(),
    }
}

fn write_results(path: &PathBuf, results: &[BenchmarkResult]) {
    let json = serde_json::to_string_pretty(results).expect("Failed to serialize results");
    std::fs::write(path, json).expect("Failed to write results file");
}
