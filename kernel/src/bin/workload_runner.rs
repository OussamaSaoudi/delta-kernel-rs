//! Standalone CLI binary for running delta-kernel-rs benchmarks.
//!
//! This binary reads workload specifications from a directory (generated by
//! the Java WorkloadSpecGenerator in Quicksilver) and runs them against
//! Delta tables using the default engine. Results are written as a JSON
//! array to an output file, matching the `KernelBenchmarkResult` schema
//! expected by Quicksilver.
//!
//! Usage:
//! ```bash
//! workload-runner \
//!   --workload-dir /path/to/specs \
//!   --warmup 1 \
//!   --iterations 3 \
//!   --output /tmp/results.json
//! ```

use std::path::PathBuf;
use std::sync::Arc;
use std::time::Instant;

use clap::Parser;
use delta_kernel::benchmarks::models::{ReadOperationType, WorkloadSpecType};
use delta_kernel::benchmarks::{create_runner, load_all_workloads};
use delta_kernel::engine::default::executor::tokio::TokioBackgroundExecutor;
use delta_kernel::engine::default::DefaultEngine;
use serde::Serialize;

/// Delta Kernel Rust workload benchmark runner.
///
/// Reads workload specs from a directory and runs benchmarks against Delta tables.
/// Outputs results as JSON matching the Quicksilver KernelBenchmarkResult schema.
#[derive(Parser, Debug)]
#[command(name = "workload-runner")]
struct Args {
    /// Directory containing workload specifications (table_info.json + specs/)
    #[arg(long)]
    workload_dir: PathBuf,

    /// Number of warmup iterations (not measured)
    #[arg(long, default_value_t = 1)]
    warmup: u32,

    /// Number of measured iterations
    #[arg(long, default_value_t = 3)]
    iterations: u32,

    /// Output file path for JSON results
    #[arg(long)]
    output: PathBuf,
}

/// Benchmark result matching Quicksilver's KernelBenchmarkResult exactly.
#[derive(Debug, Serialize)]
#[serde(rename_all = "camelCase")]
struct BenchmarkResult {
    workload_name: String,
    spec_type: String,
    durations_ms: Vec<i64>,
    min_duration_ms: i64,
    max_duration_ms: i64,
    avg_duration_ms: f64,
    std_dev_duration_ms: f64,
    success: bool,
    error_message: Option<String>,
    custom_metrics: std::collections::HashMap<String, f64>,
}

/// Set up the default engine for benchmarks.
fn setup_engine() -> Arc<DefaultEngine<TokioBackgroundExecutor>> {
    use delta_kernel::engine::default::storage::store_from_url;
    use delta_kernel::try_parse_uri;

    let dummy_url = try_parse_uri(".").expect("Failed to parse current directory");
    let store = store_from_url(&dummy_url).expect("Failed to create store");
    Arc::new(DefaultEngine::new(store))
}

/// Normalize s3a:// URIs to s3:// since `object_store` uses s3://.
fn normalize_table_path(path: &str) -> String {
    if path.starts_with("s3a://") {
        format!("s3://{}", &path[6..])
    } else {
        path.to_string()
    }
}

fn compute_stats(durations: &[i64]) -> (i64, i64, f64, f64) {
    if durations.is_empty() {
        return (0, 0, 0.0, 0.0);
    }
    let min = *durations.iter().min().unwrap();
    let max = *durations.iter().max().unwrap();
    let sum: i64 = durations.iter().sum();
    let avg = sum as f64 / durations.len() as f64;
    let variance = durations
        .iter()
        .map(|&d| {
            let diff = d as f64 - avg;
            diff * diff
        })
        .sum::<f64>()
        / durations.len() as f64;
    let stddev = variance.sqrt();
    (min, max, avg, stddev)
}

fn main() {
    let args = Args::parse();

    eprintln!("=== Delta Kernel Rust Workload Runner ===");
    eprintln!("  Workload dir: {}", args.workload_dir.display());
    eprintln!("  Warmup: {}", args.warmup);
    eprintln!("  Iterations: {}", args.iterations);
    eprintln!("  Output: {}", args.output.display());

    // Load all workload specs
    let specs = match load_all_workloads(&args.workload_dir) {
        Ok(specs) => specs,
        Err(e) => {
            eprintln!("Failed to load workload specs: {}", e);
            let results = vec![BenchmarkResult {
                workload_name: "all".to_string(),
                spec_type: "unknown".to_string(),
                durations_ms: vec![],
                min_duration_ms: 0,
                max_duration_ms: 0,
                avg_duration_ms: 0.0,
                std_dev_duration_ms: 0.0,
                success: false,
                error_message: Some(format!("Failed to load specs: {}", e)),
                custom_metrics: std::collections::HashMap::new(),
            }];
            write_results(&args.output, &results);
            std::process::exit(1);
        }
    };

    eprintln!("Loaded {} workload spec(s)", specs.len());

    let engine = setup_engine();
    let mut results = Vec::new();

    for spec in specs {
        // For Read specs, expand into operation-type variants
        match &spec.spec_type {
            WorkloadSpecType::Read(read_spec) => {
                // Use the operation_type from the spec JSON, or default to read_metadata
                let op_type = read_spec
                    .operation_type
                    .as_deref()
                    .unwrap_or("read_metadata");

                let operation = match op_type {
                    "read_metadata" => ReadOperationType::ReadMetadata,
                    "read_data" => {
                        eprintln!("Skipping read_data (not yet implemented)");
                        continue;
                    }
                    other => {
                        eprintln!("Unknown read operation type: {}", other);
                        continue;
                    }
                };

                let mut spec_with_op = spec.clone();
                spec_with_op.operation_type = Some(operation);

                // Normalize s3a:// paths
                if let Some(ref path) = spec_with_op.table_info.table_path {
                    let normalized = normalize_table_path(path);
                    if normalized != *path {
                        eprintln!("  Normalized path: {} -> {}", path, normalized);
                        spec_with_op.table_info.table_path = Some(normalized);
                    }
                }

                let result = run_benchmark(&spec_with_op, engine.clone(), &args);
                results.push(result);
            }
            WorkloadSpecType::SnapshotConstruction(_) => {
                let mut spec = spec;

                // Normalize s3a:// paths
                if let Some(ref path) = spec.table_info.table_path {
                    let normalized = normalize_table_path(path);
                    if normalized != *path {
                        eprintln!("  Normalized path: {} -> {}", path, normalized);
                        spec.table_info.table_path = Some(normalized);
                    }
                }

                let result = run_benchmark(&spec, engine.clone(), &args);
                results.push(result);
            }
        }
    }

    write_results(&args.output, &results);
    eprintln!("=== Done: {} result(s) written to {} ===", results.len(), args.output.display());
}

fn run_benchmark(
    spec: &delta_kernel::benchmarks::WorkloadSpecVariant,
    engine: Arc<DefaultEngine<TokioBackgroundExecutor>>,
    args: &Args,
) -> BenchmarkResult {
    let workload_name = spec.full_name();
    let spec_type = match &spec.spec_type {
        WorkloadSpecType::Read(_) => "read",
        WorkloadSpecType::SnapshotConstruction(_) => "snapshot_construction",
    };

    eprintln!("Running benchmark: {} (type={})", workload_name, spec_type);

    let mut runner = match create_runner(spec.clone(), engine) {
        Ok(r) => r,
        Err(e) => {
            eprintln!("  Failed to create runner: {}", e);
            return BenchmarkResult {
                workload_name,
                spec_type: spec_type.to_string(),
                durations_ms: vec![],
                min_duration_ms: 0,
                max_duration_ms: 0,
                avg_duration_ms: 0.0,
                std_dev_duration_ms: 0.0,
                success: false,
                error_message: Some(format!("Failed to create runner: {}", e)),
                custom_metrics: std::collections::HashMap::new(),
            };
        }
    };

    // Warmup iterations
    for i in 0..args.warmup {
        eprintln!("  Warmup {}/{}", i + 1, args.warmup);
        if let Err(e) = runner.setup() {
            eprintln!("  Setup failed during warmup: {}", e);
            return error_result(workload_name, spec_type, &format!("Setup failed: {}", e));
        }
        if let Err(e) = runner.execute() {
            eprintln!("  Execute failed during warmup: {}", e);
            return error_result(workload_name, spec_type, &format!("Execute failed: {}", e));
        }
        let _ = runner.cleanup();
    }

    // Measured iterations
    let mut durations_ms = Vec::with_capacity(args.iterations as usize);
    for i in 0..args.iterations {
        if let Err(e) = runner.setup() {
            eprintln!("  Setup failed at iteration {}: {}", i + 1, e);
            return error_result(workload_name, spec_type, &format!("Setup failed: {}", e));
        }

        let start = Instant::now();
        if let Err(e) = runner.execute() {
            eprintln!("  Execute failed at iteration {}: {}", i + 1, e);
            return error_result(workload_name, spec_type, &format!("Execute failed: {}", e));
        }
        let elapsed = start.elapsed();
        let ms = elapsed.as_millis() as i64;
        durations_ms.push(ms);
        eprintln!("  Iteration {}/{}: {}ms", i + 1, args.iterations, ms);

        let _ = runner.cleanup();
    }

    let (min, max, avg, stddev) = compute_stats(&durations_ms);

    eprintln!(
        "  Result: avg={}ms, min={}ms, max={}ms, stddev={:.2}ms",
        avg as i64, min, max, stddev
    );

    BenchmarkResult {
        workload_name,
        spec_type: spec_type.to_string(),
        durations_ms,
        min_duration_ms: min,
        max_duration_ms: max,
        avg_duration_ms: avg,
        std_dev_duration_ms: stddev,
        success: true,
        error_message: None,
        custom_metrics: std::collections::HashMap::new(),
    }
}

fn error_result(workload_name: String, spec_type: &str, message: &str) -> BenchmarkResult {
    BenchmarkResult {
        workload_name,
        spec_type: spec_type.to_string(),
        durations_ms: vec![],
        min_duration_ms: 0,
        max_duration_ms: 0,
        avg_duration_ms: 0.0,
        std_dev_duration_ms: 0.0,
        success: false,
        error_message: Some(message.to_string()),
        custom_metrics: std::collections::HashMap::new(),
    }
}

fn write_results(path: &PathBuf, results: &[BenchmarkResult]) {
    let json = serde_json::to_string_pretty(results).expect("Failed to serialize results");
    std::fs::write(path, json).expect("Failed to write results file");
}
